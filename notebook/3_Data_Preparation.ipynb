{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load the \"autoreload\" extension so that code can change\n",
    "# Always reload modules so that as soon as code changes in src, it gets automatically reloaded without kernel relaunch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "\n",
    "from src.utils import datacollector\n",
    "from src.utils import constants as cst\n",
    "\n",
    "from src.preprocessing import cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMINDER - OBJECTIVE\n",
    "As a reminder, **our objective for modeling will be to be able to predict the price for a given listing**.  \n",
    "We have seen that the price can be rather different depending on the period of the year. Of course, it can varies due to some other features, each one with its own importance. We have observed for example that the neighbourhood is one of them.  \n",
    "To build something which will more reflect what happens in real life, I should merge the `listings.csv.gz` dataset with the `calendar.csv.gz` one but it will give me a too huge dataset with more than 20 millions of rows and hundred of features.  \n",
    "**For computation reasons and as the goal of this project is more to validate an approach than building the perfectly perfect model, I choose to keep only the `listings.csv.gz` dataset and its 64K rows and my goal will be to try to predict the mean price for each element of the listing**.\n",
    "\n",
    "In next sections below you will find all preliminary steps that are mandatory before starting the modeling phase.  \n",
    "This will be based on what we have discovered during the [Data Understanding](1_Data_Understanding.ipynb) phase (see the 'summary' part for a reminder).\n",
    "\n",
    "# 1. Load dataset & drop unnecessary features\n",
    "The easiest thing to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst_full = pd.read_csv(datacollector.get_data_file(cst.LISTING_FULL_FILE), sep=',', header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols(df, cols_to_drop):\n",
    "    df_lst_reduced = df.drop(cols_to_drop, axis=1)\n",
    "    # Coherence control\n",
    "    assert df_lst_reduced.shape[1] == df.shape[1] - len(cols_to_drop)\n",
    "    print(\"After column dropping, new shape is now {}\".format(df_lst_reduced.shape))\n",
    "    return df_lst_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After column dropping, new shape is now (64293, 51)\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['listing_url', 'scrape_id', 'last_scraped', 'experiences_offered', 'notes', 'transit', 'interaction', \n",
    "                'house_rules', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_name', \n",
    "                'host_about', 'host_response_time', 'host_response_rate', 'host_acceptance_rate', 'host_since', \n",
    "                'host_location', 'host_neighbourhood', 'host_listings_count', 'host_total_listings_count', \n",
    "                'host_verifications', 'host_url', 'host_thumbnail_url', 'host_picture_url', 'host_has_profile_pic', \n",
    "                'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', \n",
    "                'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms', 'street', \n",
    "                'neighbourhood', 'neighbourhood_group_cleansed', 'city', 'state', 'zipcode', 'market', 'smart_location', \n",
    "                'country_code', 'country', 'latitude', 'longitude', 'property_type', 'square_feet', 'has_availability', \n",
    "                'calendar_updated', 'calendar_last_scraped', 'first_review', 'last_review', 'requires_license', \n",
    "                'is_business_travel_ready', 'require_guest_profile_picture', 'require_guest_phone_verification']\n",
    "df_lst_reduced = drop_cols(df_lst_full, cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features transformation\n",
    "## Text extraction or drop ?\n",
    "I said in [Data Understanding](1_Data_Understanding.ipynb) conclusion that depending on the use case, perhaps we could try to extract some keywords from `name`, `summary`, `space`, `description`, `neighborhood_overview`, `access`.  \n",
    "We are not in NLP problem, as those features contains only text which is moreover sometimes in french sometimes in english I propose to start with a simple approach and just drop those features also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After column dropping, new shape is now (64293, 45)\n"
     ]
    }
   ],
   "source": [
    "txt_cols_to_drop = ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'access']\n",
    "df_lst_reduced_notxt = drop_cols(df_lst_reduced, txt_cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From 't'/'f' to binary 0/1\n",
    "Here we will deal with columns that contains the now famous \"t/f\" categorical nominal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64293, 45)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cols = ['host_is_superhost', 'host_identity_verified', 'is_location_exact', 'instant_bookable']\n",
    "for feat in  tf_cols:\n",
    "    df_lst_reduced_notxt = cleaning.transform_t_f(df_lst_reduced_notxt, feat)\n",
    "df_lst_reduced_notxt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    56276\n",
       "1.0     7953\n",
       "Name: host_is_superhost, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lst_reduced_notxt.host_is_superhost.value_counts()\n",
    "# TODO il aurait fallu recuperer les valeurs avant la transfo, les stocker puis assert apres transfo que la somme des 1 = la value_counts des 't' par exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also the `license` feature to deal with: create a new binary feature which will stand for 'license provided yes/no'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle rows with missing value\n",
    "* 'bathrooms', 'bedrooms', 'beds': handle missing values (imputation, replace with 0 ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle 'currency' columns\n",
    "* price elements ('price', 'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee', 'extra_people'): remove currency symbol/Handle thousands separator and convert to float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hot encoding of categorical features\n",
    "* 'neighbourhood_cleansed': perform 1-hot encoding over the 20 values\n",
    "* 'room_type': perform 1-hot encoding over the 3 values\n",
    "* 'bed_type': perform 1-hot encoding over the 5 values. No missing value.\n",
    "* 'cancellation_policy': perform 1-hot encoding over the 6 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but not least, specific treatment\n",
    "* 'amenities': extract useful information and depending on the number of distinct values, perform 1-hot encoding over the values\n",
    "* 'jurisdiction_names': administrative information about Paris. Try to extract the name then transform as dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udadsnd-p4] *",
   "language": "python",
   "name": "conda-env-udadsnd-p4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
